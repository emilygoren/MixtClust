% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/main.R, R/zzz.R
\name{MixtClust}
\alias{MixtClust}
\title{Robust Clustering for Complete and Incomplete Data}
\usage{
MixtClust(
  x,
  initial.values = "emEM",
  nclusters = NULL,
  max.iter = 1000,
  tol = 0.001,
  convergence = "aitkens",
  sigma.constr = FALSE,
  df.constr = FALSE,
  approx.df = TRUE,
  method = "marginalization",
  verbose = TRUE,
  scaled = TRUE,
  emEM.args = list(nstarts = nclusters * 10 * prod(dim(x)), em.iter = 5, nbest = 4)
)
}
\arguments{
\item{x}{A matrix with \eqn{n} observations (rows), \eqn{p} columns
(dimensions), and missing entries set to \code{NA}.}

\item{initial.values}{Either \code{"emEM"} specifiying the emEM
initialization strategy (see \code{emEM.args} for additional arguments),
\code{"kmeans"} specifiying use of kmeans to generate an initial partition,
a vector of integers specifying an initial partition,
or a named list of initial parameter values (see details).}

\item{nclusters}{Positive integer. The assumed number of clusters if initial
values are not provided.}

\item{max.iter}{Positive integer. The maximum number of EM iterations
allowed.}

\item{tol}{Positive scalar. The desired stopping criterion value.}

\item{convergence}{Either \code{"lop"} specifying use of relative change
in loglikelihood as the convergence criterion, or \code{"aitkens"} specifying 
Aitken's acceleration (default).}

\item{sigma.constr}{Logical. Should the dispersion matrices \eqn{\Sigma_k} be
held constant over \eqn{k = 1,\dots,K} all clusters?}

\item{df.constr}{Logical. Should the degrees of freedom \eqn{\nu_k} be held
constant over \eqn{k = 1,\dots,K} all clusters?}

\item{approx.df}{Logical. If \code{approx.df = TRUE}, a numerical
approximation of the objective function used to estimate the degrees of
freedom \eqn{\nu_k} for \eqn{k = 1,\dots,K}.}

\item{method}{How should missing entries be handled? Must be either 
\code{"fullEM"} to include missing entries in the EM algorithm following Lin 2009,
 \code{"marginalization"} to integrating out missing entries, or \code{"deletion"}
 to analyze complete cases only.}

\item{verbose}{Logical. Should progress be periodically reported to the
screen?}

\item{scaled}{Logical variable that indicates if computations for multi-dimensional datasets should be done after scaling the dataset. Note that the resulting parameters are scaled back and so should not theoretically have much effect on the performance, except to potentially offer stability in numerical computations.}

\item{emEM.args}{A named list of options utilized if \code{initial.values =
"emEM"} (see details).}
}
\value{
A list containing: \itemize{ \item{"estimates"}{ A list of the final
  estimates "pi", "nu", "mu", and "Sigma" containing the MLEs for the mixing
  proportions, degrees of freedom, locations, and dispersions, respectively.}
  \item{"iterations"}{ Number of EM iterations performed (long EM run only;
  if emEM was performed, this excludes the short em run iterations specified
  in emEM.args$em.iter).} \item{"Zs"}{ A \eqn{n \times K} matrix where the
  \eqn{i}-th row contains the posterior probabilities of membership in
  cluster \eqn{1, \dots, K} for the \eqn{i}-th observation
  (\eqn{i=1,\dots,n}).} \item{"class"}{ A vector of length \eqn{n} with the
  predicted class memberships for each observation.} \item{"loglik"}{ The
  log likelihood at each (long EM run) iteration.} \item{"loglik"}{The 
  log likelihood at the last iteration, computed for all cases (including 
  those with missing values when \code{method = "deletion"})}
  \item{"bic"}{ The BIC for
  the final fitted model.} \item{"EM.time"}{ Runtime for the long EM run(s).}
  \item{"em.time"}{ Runtime for the short em run(s) when \code{initial.values
  = "emEM"}.} \item{"total.time"}{ Runtime for the entire function call.}
  \item{"call"}{ Supplied function call.} \code{npar}{The number of model parameters.}}
}
\description{
Clustering using finite mixture of multivariate t distributions
  including handling of incomplete data.

Robust clustering, including handling of incomplete data, using the EM algorithm for 
  finite mixtures of multivariate t distributions
}
\details{
Model-based clustering using finite mixtures of t distributions, with
handling of incomplete data using either marginalization or the EM algorithm.
If supplying initial values, format as a named list with elements: \itemize{
\item{"pi"}{ Mixing proportions. A vector of length \eqn{K} that sums to
one.} \item{"nu"}{ Degrees of freedom. A vector of length \eqn{K} with
entries at least equal to three (thus requiring the existance of the first
two moments.)} \item{"mu"}{ Locations. A \eqn{K \times p} matrix, where the
\eqn{k}-th row is the location \eqn{\mu_k \in R^p} for cluster \eqn{k}.}
\item{"Sigma"}{ Dispersions. A \eqn{p \times p \times K} array, where the
\eqn{k}-th slice is the \eqn{p \times p} positive-definite dispersion matrix
\eqn{\Sigma_k} for cluster \eqn{k}.} }

The arguments for emEM specified in the list \code{emEM.args} are: \itemize{
\item{"nstarts"}{ Positive integer. The number of randomly generated initial
starting parameter values under consideration.} \item{"em.iter"}{ Positive
integer. The number of short EM iterations to be performed on each set of
initial starting parameter values.} \item{"nbest"}{ Positive integer. After
\code{em.iter} EM iterations are performed in each of the \code{nstarts}
initial values, the number of top ranking (according to loglikelihood)
parameter values on which to run the long EM either to convergence (specified
by \code{tol}) or maximum number of iterations (specified by
\code{max.iter}). If \code{nbest} is greater than one, the long EM run
achieving the largest loglikelihood will be returned.} }

Model-based clustering using finite mixtures of t distributions, with handling of incomplete data using either marginalization or the EM algorithm.
}
\examples{
set.seed(20180626)
# Use iris data.
d <- subset(iris, select = -Species)
# Create missing data -- MCAR with 10\% chance of missingness.
missing <- matrix(rbinom(n = ncol(d)*nrow(d), size = 1, prob = 0.1), ncol = ncol(d))
x <- d; x[missing == 1] <- NA
# Run EM with emEM initialization strategy for candidate clusters K = 2, 3, 4.
Ks <- 2:4
ans <- lapply(Ks, function(K) {
    MixtClust(x, nclusters = K, emEM.args = list(nstarts=K*10, em.iter=5, nbest=1))
})
# Get BIC for each K.
BICs <- sapply(ans, function(f) f$bic)
# Plot BIC by K.
plot(BICs ~ Ks, pch = 20, xlab = 'Number of Clusters', ylab = 'BIC')

}
\references{
Emily M. Goren & Ranjan Maitra, 2022. "Fast model-based clustering of partial records," Stat, 11(1), e416. https://doi.org/10.1002/sta4.416"

Tsung-I Lin & Hsiu Ho & Pao Shen, 2009. "Computationally
efficient learning of multivariate t mixture models with missing
information," Computational Statistics, 24(3): 375-392.
}
\author{
Emily Goren, \email{emily.goren@gmail.com}
}
